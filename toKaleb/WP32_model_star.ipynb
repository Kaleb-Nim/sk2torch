{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-fa08b5f17a75>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mWP32_utils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\toKaleb\\WP32_utils.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 130\u001B[1;33m \u001B[1;32mclass\u001B[0m \u001B[0mdilatedCNN\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mModule\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    131\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mD_in\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mD_out\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mKERNEL_SIZE\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mT\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mHorizon\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    132\u001B[0m         \"\"\"\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from WP32_utils import *\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using fixed random seed = 100 to duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random_state = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# friction coefficients μ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import nan\n",
    "μ = 1\n",
    "Lx = 0.2775\n",
    "Ly = 0.26\n",
    "filename = os.path.join('Path Data', 'all_data.csv')\n",
    "df = pd.read_csv(filename)\n",
    "v, w, payload, unit_weight, m = df['v1'].values, df['w1'].values, df['payload'].values, df['unit weight'].values, df['m'].values\n",
    "M = payload*unit_weight+m #df['payload']*df['unit weight'] + df['m']#all weight\n",
    "r = np.array([vv/ww if ww!=0 else nan for vv, ww in zip(v, w)])\n",
    "# P_ul = μ*M*g*v\n",
    "P_ul= np.array([μ*MM*9.8*vv if ww == 0 else 0 for ww, vv, MM in zip(w, v, M)])\n",
    "wheel_v1, wheel_v2 = np.array([np.hypot((rr-Lx), Ly)*ww if ww!=0 else vv for ww, vv, rr in zip(w, v, r)]),  np.array([np.hypot((rr-Lx), Ly)*ww if ww!=0 else vv for ww, vv, rr in zip(w, v, r)])\n",
    "wheel_v3, wheel_v4 = np.array([np.hypot((rr+Lx), Ly)*ww if ww!=0 else vv for ww, vv, rr in zip(w, v, r)]),  np.array([np.hypot((rr+Lx), Ly)*ww if ww!=0 else vv for ww, vv, rr in zip(w, v, r)]),\n",
    "df['M'] = M\n",
    "df['r'] = r\n",
    "df['P_ul'] = P_ul\n",
    "df['wheel_v1'], df['wheel_v2'], df['wheel_v3'], df['wheel_v4'] = wheel_v1, wheel_v2, wheel_v3, wheel_v4\n",
    "P_ut = np.array([1/4*μ*MM*9.8*(np.abs(wv1)+np.abs(wv2)+np.abs(wv3)+np.abs(wv4)) if ww !=0 else 0 for ww, MM, wv1, wv2, wv3, wv4, in zip(w, M, wheel_v1, wheel_v2, wheel_v3, wheel_v4)])\n",
    "df['P_ut'] = P_ut\n",
    "df['P_useful'] = P_ut + P_ul\n",
    "#add knowledge part\n",
    "df['w2'] = df['w1']*df['w1']\n",
    "df['v2'] = df['v1']*df['v1']\n",
    "df['w3'] = df['w1']*df['w1']*df['w1']\n",
    "df['v3'] = df['v1']*df['v1']*df['v1']\n",
    "df['w4'] = df['w1']*df['w1']*df['w1']*df['w1']\n",
    "df['v4'] = df['v1']*df['v1']*df['v1']*df['v1']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[['P_useful', 'Pstable']].plot()\n",
    "df.to_csv('results_all.csv')\n",
    "df.to_json('Results/results.json', orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "num_hidden_layers = (120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Hybrid modelling - model *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size_factor = 1\n",
    "max_iters = [i for i in range(1000, 15000, 1000)]\n",
    "c = ['v1', 'w1', 'payload', 'P_ul', 'P_ut']\n",
    "\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "results= []\n",
    "mape_best = 1\n",
    "for max_iter in max_iters:\n",
    "    r = []\n",
    "    #balanced training\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "    for pl in range(3):\n",
    "        df_xy = df[df['payload'] == pl]\n",
    "        X_part, y_part = df_xy[c].to_numpy(), df_xy['Pstable'].to_numpy().reshape(-1)\n",
    "        X_train_part, X_test_part, y_train_part, y_test_part = train_test_split(X_part, y_part, random_state=random_state, test_size=test_size)\n",
    "        # print(X_train_part.shape, X_test_part.shape, y_train_part.shape, y_test_part.shape)\n",
    "        X_train.append(X_train_part)\n",
    "        y_train.append(y_train_part)\n",
    "        X_test.append(X_test_part)\n",
    "        y_test.append(y_test_part)\n",
    "\n",
    "    X_train = np.vstack(X_train)\n",
    "    y_train = np.block(y_train)\n",
    "    X_test = np.vstack(X_test)\n",
    "    y_test = np.block(y_test)\n",
    "\n",
    "    #apply size factor\n",
    "    X_train = X_train[:int(data_size_factor*X_train.shape[0])]\n",
    "    y_train = y_train[:int(data_size_factor*y_train.shape[0])]\n",
    "    X_test = X_test[:int(data_size_factor*X_test.shape[0])]\n",
    "    y_test = y_test[:int(data_size_factor*y_test.shape[0])]\n",
    "    # train\n",
    "    \n",
    "    t0 = time()\n",
    "    regr = MLPRegressor(random_state=1,hidden_layer_sizes=num_hidden_layers, max_iter=max_iter).fit(X_train, y_train)\n",
    "    if max_iter == 4000:\n",
    "        model_filename = 'model_star.pickle'\n",
    "        print(\"saving=================\")\n",
    "        pickle.dump(regr, open(os.path.join(\"Results\", model_filename), 'wb'))\n",
    "\n",
    "    y_pred_train = regr.predict(X_train)\n",
    "    y_pred_test = regr.predict(X_test)\n",
    "    r.append(r2_score(y_test, y_pred_test))\n",
    "    r.append(MAPE(y_test, y_pred_test))\n",
    "    r.append(time()-t0)\n",
    "    print(\"R2 = %.2f, MAPE = %.3f, time = %.2fs\"%(r[0], r[1], r[2]))\n",
    "    results.append(r)\n",
    "df_results = pd.DataFrame(results, columns = ['R2', 'MAPE', 'Time'])\n",
    "df_results['MAPE'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('DRLS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "464bd29beba74e43dee38e15484eea2b25796c061a8e22578aff91c8c20f8d46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
